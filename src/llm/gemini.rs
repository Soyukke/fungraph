use std::pin::Pin;

use async_trait::async_trait;
use futures::Stream;
use log::debug;

use reqwest::header::{AUTHORIZATION, CONTENT_TYPE};
use serde::{Deserialize, Serialize};

use crate::types::StreamData;

use super::{
    LLMError, Message, MessageType,
    llm::{CallOptions, GenerateResult, LLM},
};

#[derive(Clone)]
pub enum GeminiModel {
    Gemini15,
    Gemini20,
}

impl ToString for GeminiModel {
    fn to_string(&self) -> String {
        match self {
            GeminiModel::Gemini15 => "gemini-1.5-flash".to_string(),
            GeminiModel::Gemini20 => "gemini-2.0-flash-001".to_string(),
        }
    }
}

impl Into<String> for GeminiModel {
    fn into(self) -> String {
        self.to_string()
    }
}

#[derive(Clone)]
pub struct Gemini {
    api_key: String,
    api_base: String,
    options: CallOptions,
    model: String,
}

impl Gemini {
    pub fn new(api_key: String) -> Self {
        Self {
            api_key,
            api_base: "https://generativelanguage.googleapis.com/v1beta".to_string(), // Gemini APIのベースURL
            options: CallOptions::default(),
            model: GeminiModel::Gemini15.to_string(),
        }
    }

    pub fn with_model<S: Into<String>>(mut self, model: S) -> Self {
        self.model = model.into();
        self
    }

    pub fn with_options(mut self, options: CallOptions) -> Self {
        self.options = options;
        self
    }
}

#[derive(Clone, Serialize, Default, Debug, Deserialize, PartialEq)]
#[serde(rename_all = "lowercase")]
pub enum ChatCompletionToolType {
    #[default]
    Function,
}

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct FunctionCall {
    /// The name of the function to call.
    pub name: String,
    /// The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    pub arguments: String,
}

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct ChatCompletionMessageToolCall {
    /// The ID of the tool call.
    pub id: String,
    /// The type of the tool. Currently, only `function` is supported.
    pub r#type: ChatCompletionToolType,
    /// The function that the model called.
    pub function: FunctionCall,
}

#[derive(Debug, Serialize, Deserialize, Clone, Copy, Default, PartialEq)]
#[serde(rename_all = "lowercase")]
pub enum Role {
    System,
    #[default]
    User,
    Assistant,
    Tool,
    Function,
}

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct ChatCompletionResponseMessage {
    /// The contents of the message.
    pub content: Option<String>,
    /// The refusal message generated by the model.
    pub refusal: Option<String>,
    /// The tool calls generated by the model, such as function calls.
    pub tool_calls: Option<Vec<ChatCompletionMessageToolCall>>,

    /// The role of the author of this message.
    pub role: Role,

    /// Deprecated and replaced by `tool_calls`.
    /// The name and arguments of a function that should be called, as generated by the model.
    #[deprecated]
    pub function_call: Option<FunctionCall>,
}

#[derive(Debug, Serialize, Deserialize, Clone, Copy, PartialEq)]
#[serde(rename_all = "snake_case")]
pub enum FinishReason {
    Stop,
    Length,
    ToolCalls,
    ContentFilter,
    FunctionCall,
}

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct TopLogprobs {
    /// The token.
    pub token: String,
    /// The log probability of this token.
    pub logprob: f32,
    /// A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
    pub bytes: Option<Vec<u8>>,
}

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct ChatCompletionTokenLogprob {
    /// The token.
    pub token: String,
    /// The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    pub logprob: f32,
    /// A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
    pub bytes: Option<Vec<u8>>,
    ///  List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
    pub top_logprobs: Vec<TopLogprobs>,
}

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct ChatChoiceLogprobs {
    /// A list of message content tokens with log probability information.
    pub content: Option<Vec<ChatCompletionTokenLogprob>>,
    pub refusal: Option<Vec<ChatCompletionTokenLogprob>>,
}

#[derive(Debug, Deserialize, Serialize, Clone, PartialEq)]
pub struct ChatChoice {
    /// The index of the choice in the list of choices.
    pub index: u32,
    pub message: ChatCompletionResponseMessage,
    /// The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
    /// `length` if the maximum number of tokens specified in the request was reached,
    /// `content_filter` if content was omitted due to a flag from our content filters,
    /// `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
    pub finish_reason: Option<FinishReason>,
    /// Log probability information for the choice.
    pub logprobs: Option<ChatChoiceLogprobs>,
}

#[derive(Debug, Deserialize, Serialize)]
struct GeminiResponse {
    /// A unique identifier for the chat completion.
    pub id: Option<String>,
    /// A list of chat completion choices. Can be more than one if `n` is greater than 1.
    pub choices: Vec<ChatChoice>,
    /// The Unix timestamp (in seconds) of when the chat completion was created.
    pub created: u32,
    /// The model used for the chat completion.
    pub model: String,
    /// The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
    pub service_tier: Option<String>,
    /// This fingerprint represents the backend configuration that the model runs with.
    ///
    /// Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
    pub system_fingerprint: Option<String>,

    /// The object type, which is always `chat.completion`.
    pub object: String,
    pub usage: Option<String>,
}
#[derive(Debug, Deserialize, Serialize)]
struct OpenAIContent {
    role: String,
    content: String,
}

#[derive(Debug, Deserialize, Serialize)]
struct GeminiInlineData {
    mime_type: String,
    data: String,
}

#[derive(Debug, Deserialize, Serialize)]
struct GeminiSafetyRating {
    category: String,
    probability: String,
}

#[derive(Debug, Deserialize, Serialize)]
struct GeminiPromptFeedback {
    safety_ratings: Vec<GeminiSafetyRating>,
}

#[derive(Debug, Serialize)]
struct GeminiRequest {
    messages: Vec<OpenAIContent>,
    model: String,
}

#[derive(Debug, Serialize)]
struct OpenAIRequestMessage {
    role: String,
    parts: Vec<OpenAIContent>,
}

// open ai互換のgeminiを使う
// https://developers.googleblog.com/en/gemini-is-now-accessible-from-the-openai-library/

#[async_trait]
impl LLM for Gemini {
    async fn generate(&self, prompt: &[Message]) -> Result<GenerateResult, LLMError> {
        let gemini_request = self.build_gemini_request(prompt)?;
        let client = reqwest::Client::new();
        let url = format!("{}/chat/completions", self.api_base);
        debug!("Gemini Request Url: {:?}", url);

        let response = client
            .post(&url)
            .header(CONTENT_TYPE, "application/json")
            .header(AUTHORIZATION, format!("Bearer {}", self.api_key))
            .body(serde_json::to_string(&gemini_request)?)
            .send()
            .await?;
        debug!("Gemini Response: {:?}", response);
        let status = response.status();
        let body_json = response.text().await?;
        debug!("Gemini Response Body: {:?}", body_json);

        if status.is_success() {
            let gemini_response: GeminiResponse = serde_json::from_str(&body_json)?;
            let mut generate_result = GenerateResult::default();
            if let Some(choice) = gemini_response.choices.first() {
                choice.message.content.as_ref().map(|content| {
                    generate_result.generation = content.clone();
                });
            }
            Ok(generate_result)
        } else {
            Err(LLMError::OtherError(format!(
                "Gemini API error: {} - {}",
                status, body_json
            )))
        }
    }

    async fn invoke(&self, prompt: &str) -> Result<String, LLMError> {
        self.generate(&[Message::new_human_message(prompt)])
            .await
            .map(|res| res.generation)
    }

    async fn stream(
        &self,
        messages: &[Message],
    ) -> Result<Pin<Box<dyn Stream<Item = Result<StreamData, LLMError>> + Send>>, LLMError> {
        debug!("message: {:?}", messages);
        Err(LLMError::OtherError(
            "stream not implemented for Gemini".to_string(),
        ))
    }

    fn add_options(&mut self, options: &CallOptions) {
        self.options.merge(options);
    }
}

impl Gemini {
    fn build_gemini_request(&self, messages: &[Message]) -> Result<GeminiRequest, LLMError> {
        let mut contents: Vec<OpenAIContent> = Vec::new();
        for message in messages {
            let role = match message.message_type {
                MessageType::AIMessage => "model",
                MessageType::HumanMessage => "user",
                MessageType::SystemMessage => "user", // GeminiはSystemMessageをサポートしない
                MessageType::ToolMessage => "user",   // GeminiはToolMessageをサポートしない
            }
            .to_string();

            let gemini_message = OpenAIContent {
                content: message.content.clone(),
                role: role,
            };
            contents.push(gemini_message);
        }
        let gemini_request = GeminiRequest {
            messages: contents,
            model: self.model.clone(),
        };
        debug!(
            "Gemini Request json: {:?}",
            serde_json::to_string(&gemini_request)?
        );
        Ok(gemini_request)
    }
}
